# Context
# This Algorithm implied by the pseudocode function BMM is trying to solve the problem of boolean matrix multiplication.
This is standard matrix multiplication but with boolean or in place of plus and boolean and in place of multiplication.
The potential applications of this algorithm include a bunch of graph theory things, and string cfg parsing.
#Pseudocode
let const COMPILER_CONSTANT_FOR_OPTIMIZATION = 8
let const ERROR_RATE_FOR_OPTIMIZATION = 1/5
matrix BMM(matrix A, matrix B) : # Runs with error probabilty 1/n
  let n be a power of 2 and the dimension of A,B
  let C be a n by n zeroed matrix
  for i in 1 to 3lg(n) : 
    C = C entry_wise_logical_or eBMM(A,B 1/2)
  return C
matrix eBMM( matrix A, matrix B, float error_probability) :
  let n be a power of 2 and the dimension of A,B
  if n < COMPILER_CONSTANT_FOR_OPTIMIZAITON:
    return witness_listing_BMM(A,B)
  if error_proability < 1/4 : # WEIRD SELF REDUCTION
    return eBMM(A,B,sqrt(error_probability)) entry_wise_logical_or eBMM(A,B,sqrt(error_probability))
  # NOW WE ESTIMATE THE WITNESS DISTRIBUTION
  let s be a random list of n pairs of integers from 1 to n
  let w be an array of zeros of length n +1 (goes from 0 to n)
  for (i,j) in s :
    witness_count = (row i of A) dot product (column j of B)
    w[witness_count] += 1
  # THEN USE THAT TO COMPUTE THE error estimate
  error_estimate = 0
  for i in 1 to n :
    error_estimate += n * w[i] * 2^(-i) 
  if error_estimate < n^2 *error_proability* ERROR_RATE_FOR_OPTIMIZATION :
    return dense_divide_BMM(A,B,error_probability * (1- ERROR_RATE_FOR_OPTIMIZATION) ) :
  else:
    return witness_listing_BMM(A,B)
matrix dense_divide_BMM(matrix A, matrix B, float error_rate_after_dropping)
  let n be a power of 2 and the dimension of A,B
  let t be a random list of distinct integers between 1 and n of length n/2
  let A_1 be the first n/2 rows of A with columns corresponding to t
  let A_2 be the last n/2 rows of A with columns corresponding to t
  let B_1 be the first n/2 columns of B with rows corresponding to t
  let B_2 be the last n/2 columns of B with rows corresponding to t
  C_11 = eBMM( A_1, B_1, error_rate_after_dropping)
  C_12 = eBMM( A_1, B_2, error_rate_after_dropping)
  C_21 = eBMM( A_2, B_1, error_rate_after_dropping)
  C_22 = eBMM( A_2, B_2, error_rate_after_dropping)
  return [C_11, C_12;
          C_21, C_22]
matrix witness_listing_BMM( matrix A, matrix B) 
  # NOTE : I explained this method using graph theory terms, but its the same as the lightning algorithm
  let n be a power of 2 and the dimension of A,B
  Let A be a directed adjacency matrix, and construct its adjacency list A'
  Let B be a directed adjacency matrix, and construct its adjacency list B'
  Let C be an zeroed n by n matrix
  for each vertex v of A:
    for each outgoing edge (v,u)  of v in A' :
      for each outgoing edge(u,w) of u in B' :
        set entry v,w to 1 in C
  return C
# END PSEUOCODE
#NOTE 1/5 is arbitrary, and causes a double roughly every 3 times
# varying ERROR_RATE_FOR_OPTIMIZATION improves the critical exponent of this recursive algorithm as
# analyzed by the master theorem, but worsens the constant for the non-recursive base case of witness listing.
# this constant can interestingly vary between (0,1/2) non inclusive
# Dynamically changing the error rate based on input size seems feasible but hard.

